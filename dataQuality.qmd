---
title: "MIS Data Quality"
author: "John R. Foster"
format: docx
editor: visual
execute: 
  warning: false
  error: false
  echo: false
---

## Executive summary





```{r}
#| label: setup

library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(knitr)

#----Required Functions
source("../pigs-property/R/functions_data.R")
source("R/FNC.Misc.Utilities.R")

#----get correct data pull----
pull.date <- config::get("pull.date")
pull.date.ymd <- dmy(pull.date)

#---- read path ----
read.path <- "data/raw"

#---- write path ----
write.path <- "data/processed"
processed <- "processed_"

#--Property Data
csv.name <- paste0("fs_national_take_by_property_", pull.date, ".csv")
file.name <- file.path(read.path, csv.name)
df <- read.csv(file.name, stringsAsFactors = FALSE) 
dat.Agr.take <- df |>
  as_tibble() |> 
  distinct() |>
  select(-PRPS_QTY) |>
  mutate(AGRPROP_ID = WT_AGRPROP_ID)

n_raw_records <- nrow(dat.Agr.take)
min_date <- min(dmy(dat.Agr.take$WT_WORK_DATE))
max_date <- max(dmy(dat.Agr.take$WT_WORK_DATE))

csv.name <- paste0("fs_national_property_", pull.date, ".csv")
file.name <- file.path(read.path, csv.name)
df <- read.csv(file.name, stringsAsFactors = FALSE)
dat.Agr.property <- df |>
  as_tibble() |> 
  distinct() |>
  group_by(AGRP_PRP_ID, ALWS_AGRPROP_ID, ALWS_DA_ID, PRP_NAME, ST_NAME, 
           ST_GSA_STATE_CD, CNTY_NAME, CNTY_GSA_CNTY_CD, PRPS_PROP_TYPE) |>
  filter(PRPS_QTY == max(PRPS_QTY)) |> # Assume max PRPS_QTY is property size
  ungroup() |>
  mutate(AGRPROP_ID = ALWS_AGRPROP_ID)

data_farm_bill <- read_csv("data/All_FB_Agreements_long_2024-05-30.csv")

farm_bill_properties <- data_farm_bill |> 
  rename(ALWS_AGRPROP_ID = propertyID) |> 
  select(-agreement_name, -property_name) |> 
  mutate(farm_bill = 1)

dat.Agr <- left_join(dat.Agr.take, dat.Agr.property)
dat.Agr <- left_join(dat.Agr, farm_bill_properties)
dat.Agr <- dat.Agr |> 
  mutate(propertyID = paste0(AGRP_PRP_ID, "-", WT_AGRPROP_ID))

n_with_pigs <- length(unique(dat.Agr$propertyID))

n_farm1 <- dat.Agr |> 
  filter(farm_bill == 1) |> 
  pull(propertyID) |> 
  unique() |> 
  length()

n_no_state <- dat.Agr |> 
  filter(is.na(ST_GSA_STATE_CD)) |> 
  pull(propertyID) |> 
  unique() |> 
  length()

loss_1 <- round(n_no_state / n_with_pigs * 100, 2)

dat.Agr2 <- dat.Agr[complete.cases(dat.Agr$ST_GSA_STATE_CD),]

# the number of properties that have an area listed as 1 acre
n_1acre <- dat.Agr2 |> 
  filter(PRPS_QTY == 1 | is.na(PRPS_QTY)) |>
  pull(propertyID) |> 
  unique() |> 
  length()

dat.Agr2 <- dat.Agr2 |> 
  filter(PRPS_QTY != 1)

loss_2 <- round(n_1acre / n_with_pigs * 100, 2)

n_possible <- length(unique(dat.Agr2$propertyID))

good_props <- dat.Agr |> 
  filter(farm_bill == 1,
         !is.na(ST_GSA_STATE_CD),
         PRPS_QTY > 1) |> 
  pull(propertyID) |> 
  unique() 

n_farm2 <- length(good_props)


dat.Eff <- read_csv(file.path(write.path, paste0("processed_fs_national_effort_", pull.date, ".csv")))
dat.Agr <- read_csv(file.path(write.path, paste0("processed_fs_national_property_", pull.date, ".csv")))

get_numbers <- function(df_raw, df_processed){
  out <- list()
  out$n_rows_raw <- nrow(df_raw)
  out$n_rows_processed <- nrow(df_processed)
  out$n_rows_lost <- nrow(df_raw) - nrow(df_processed)
  out$percent_rows_lost <- round(out$n_rows_lost / nrow(df_raw) * 100, 2)  
  out$n_props_raw <- length(unique(df_raw$AGRP_PRP_ID))
  out$n_props_processed <- length(unique(df_processed$AGRP_PRP_ID))
  out$n_props_lost <- out$n_props_raw - out$n_props_processed
  out$percent_props_lost <- round(out$n_props_lost / out$n_props_raw * 100, 2)
  return(out)
}
```

```{r}
#| label: traps

dat.Agr<-dat.Agr[dat.Agr$DA_NAME_TYPE %not in% c("small mammal","rodent"),]

dat.Agr<-aggregate(PRPS_QTY ~ AGRP_PRP_ID+ALWS_AGRPROP_ID+ALWS_DA_ID+PRP_NAME+ST_NAME+ST_GSA_STATE_CD+CNTY_NAME+CNTY_GSA_CNTY_CD+PRPS_PROP_TYPE, data=dat.Agr, FUN=max)

dat.Eff$unk.id <- paste0(dat.Eff$AGRP_PRP_ID,".",dat.Eff$ALWS_AGRPROP_ID)
dat.Agr$unk.id <- paste0(dat.Agr$AGRP_PRP_ID,".",dat.Agr$ALWS_AGRPROP_ID)
dat.Eff <- dat.Eff[dat.Eff$unk.id %in% unique(dat.Agr$unk.id),]

cmp.vec<-c("RAMIK MINI BARS :HI-9 (ANTICOAG)","TRAPS, SNAP (RAT, MOUSE, ETC.)")
prp.vec<-unique(dat.Eff[dat.Eff$CMP_NAME %in% cmp.vec,"AGRP_PRP_ID"])
trap.dat<-dat.Eff[dat.Eff$AGRP_PRP_ID %not in% prp.vec,]
trap.vec <- c("TRAPS, LIVE, FERAL HOGS","TRAPS, CAGE","TRAPS, CORRAL")
trap.raw<-trap.dat[trap.dat$CMP_NAME %in% trap.vec,]

traps.procesed <- read_csv(file.path(write.path, paste0("feral.swine.effort.take.trap.ALL.daily.events.",pull.date,".csv")))
traps <- get_numbers(trap.raw, traps.procesed)

```

The latest MIS data was pulled on `r pull.date.ymd`, and spans from `r min_date` to `r max_date`. This raw (unprocessed) data contains `r n_with_pigs` properties, which includes `r n_farm1` Farm Bill properties.

Data was processed separately for each removal method. Below is a summary for each removal method, and the main causes for dropping removal events.
  
#### Traps

- There are `r traps$n_rows_raw` records determined as trapping events, spread across `r traps$n_props_raw` properties.

- We had to drop `r traps$n_rows_lost` records (`r traps$percent_rows_lost`%) because we could not determine effort (trap nights) due to data entry errors. This was mostly due to implausible combinations of hours spent trapping, the number of traps set, and/or the number of traps checked, removed, or unset. 

  - This resulted in `r traps$n_props_lost` properties (`r traps$percent_props_lost`%) being dropped from the trapping data.

```{r}
#| label: firearms

firearms.vec <- c("FIREARMS")
tmp<-dat.Eff[dat.Eff$CMP_NAME %in% firearms.vec,]
unk.events <- unique(tmp[,c("AGRP_PRP_ID","ALWS_AGRPROP_ID","WT_WORK_DATE")])
unk.events <- cbind.data.frame(unk.events,firearms.used=rep("firearms",nrow(unk.events)))
unk.events$event.id <- seq(1, nrow(unk.events), 1)
tmp<-merge(dat.Eff, unk.events, by=c("AGRP_PRP_ID","ALWS_AGRPROP_ID","WT_WORK_DATE"), all.y=TRUE)
cmp.name <- c("SPOTLIGHT","CALLING DEVICE, MANUAL(HAND,BLOWN)","NIGHT VISION/INFRARED EQUIPMENT",
               "CALLING DEVICE, ELECTRONIC","BAIT STATION","MONITORING CAMERA","CAR/TRUCK","TELEMETRY EQUIPMENT","BAIT STATION")
event.vec<-tmp[tmp$CMP_NAME %in% cmp.name,"event.id"]
firearms.associated<-tmp[tmp$event.id %in% event.vec,]
cmp.name <- c("FIXED WING","HELICOPTER","SNARES, FOOT/LEG","SNARES, NECK",
                        "SNARES, NECK MECHANICAL (COLLARUM)","TRAPS, BODY GRIP",
                        "TRAPS, BODY GRIP","TRAPS, CAGE","TRAPS, CORRAL","TRAPS, DECOY",
                        "TRAPS, FOOTHOLD","TRAPS, FOOTHOLD (PADDED)","TRAPS, FOOTHOLD DOG PROOF",
                        "TRAPS, LIVE, FERAL HOGS","TRAPS, OTHER","TRAPS, RAPTOR (OTHER)",
                        "TRAPS, RAPTOR (SWEDISH GOSHAWK)","M-44 CYANIDE CAPSULE")
event.vec<-firearms.associated[firearms.associated$CMP_NAME %in% cmp.name,"event.id"]
firearms.associated<-firearms.associated[firearms.associated$event.id %not in% event.vec,]

firearms.procesed <- read_csv(file.path(write.path, paste0("feral.swine.effort.take.firearms.ALL.daily.",pull.date,".csv")))
firearms <- get_numbers(firearms.associated, firearms.procesed)

```

#### Firearms

- There are `r firearms$n_rows_raw` records determined as firearm/hunting events, spread across `r firearms$n_props_raw` properties.

- We had to drop `r firearms$n_rows_lost` records (`r firearms$percent_rows_lost`%) because of duplicate records, hours spent hunting was recorded as 0 or as longer than 24, or because their property ID ("AGRP_PRP_ID") was not recorded.

  - This resulted in `r firearms$n_props_lost` properties (`r firearms$percent_props_lost`%) being dropped from the firearms data.

```{r}
#| label: aerial

aerial.vec <- c("HELICOPTER","FIXED WING")
tmp<-dat.Eff[dat.Eff$CMP_NAME %in% aerial.vec,]
tmp<-tmp[tmp$UOM_NAME=="HOBBS METER",]
tmp<-tmp[tmp$USET_NAME %not in% c("DISCHARGED"),]

dat.aerial<-read.csv(file.path(write.path, paste0("feral.swine.effort.take.aerial.ALL.daily.",pull.date,".csv")))
aerial <- get_numbers(tmp, dat.aerial)

```

#### Aerial

- There are `r aerial$n_rows_raw` records determined as aerial hunting events (e.g. helicopters and fixed wing aircraft), spread across `r aerial$n_props_raw` properties.

- We had to drop `r aerial$n_rows_lost` records (`r aerial$percent_rows_lost`%) because of data entry issues such as implausible number of aircraft flown, flight duration not recorded with a Hobbs meter, or the area of the property was not recorded.

  - This resulted in `r aerial$n_props_lost` properties (`r aerial$percent_props_lost`%) being dropped from the aerial data.

```{r}
#| label: snares

dat.PropKill <- read_csv(file.path(write.path, paste0("processed_fs_national_take_by_property_", pull.date, ".csv")))

dat.PropKill$unk.id <- paste0(dat.PropKill$AGRP_PRP_ID,".",dat.PropKill$ALWS_AGRPROP_ID)
dat.Agr$unk.id <- paste0(dat.Agr$AGRP_PRP_ID,".",dat.Agr$ALWS_AGRPROP_ID)
dat.PropKill <- dat.PropKill[dat.PropKill$unk.id %in% unique(dat.Agr$unk.id),]

trap.vec <- c("SNARES, NECK","SNARES, FOOT/LEG","TRAPS, FOOTHOLD","TRAPS, BODY GRIP","SNARES, NECK MECHANICAL (COLLARUM)","TRAPS, FOOTHOLD (PADDED)","TRAPS, FOOTHOLD DOG PROOF")
dat.PropKill<-dat.PropKill[dat.PropKill$CMP_NAME %in% trap.vec,]

max.vals <- aggregate(WTCM_QTY~CMP_NAME, data=dat.PropKill, FUN=max)

trap.dat<-data.frame()

for(i in 1:nrow(max.vals)){
  tmp<-dat.Eff[dat.Eff$CMP_NAME==max.vals[i,"CMP_NAME"] & dat.Eff$WTCM_QTY < max.vals[i,"WTCM_QTY"], ]
  trap.dat<-rbind.data.frame(trap.dat,tmp)
}#END Loop

dat.snare<-read.csv(file.path(write.path, paste0("feral.swine.effort.take.snare.ALL.daily.", pull.date,".csv")))
snare <- get_numbers(trap.dat, dat.snare)

```

#### Snares

- There are `r snare$n_rows_raw` records determined as snaring events, spread across `r snare$n_props_raw` properties.

- We had to drop `r snare$n_rows_lost` records (`r snare$percent_rows_lost`%) because we could not determine effort (trap nights) due to data entry errors. This was mostly due to implausible combinations of hours spent snaring, the number of snares set, and/or the number of snares checked, removed, or unset. 

  - This resulted in `r snare$n_props_lost` properties (`r snare$percent_props_lost`%) being dropped from the trapping data.

```{r}
#| label: final
source("../pigs-property/R/functions_data.R")
library(targets)

csv.name <- paste0("MIS.Effort.Take.All.Methods.Daily.Events.", pull.date, ".csv")
file.name <- file.path("data/processed", csv.name)
dat_mis <- read_csv(file.name) |> 
  mutate(propertyID = paste0(agrp_prp_id, "-", alws_agrprop_id))

data_farm_bill <- read_csv("data/All_FB_Agreements_long_2024-05-30.csv")

n_fb_prop <- length(unique(data_farm_bill$propertyID))

farm_bill_properties <- data_farm_bill |> 
  rename(alws_agrprop_id = propertyID) |> 
  select(-agreement_name, -property_name) |> 
  mutate(farm_bill = 1)

with_fb <- dat_mis |> 
  left_join(farm_bill_properties)

n_processed <- length(unique(with_fb$propertyID))
process_left <- round(n_processed / n_possible * 100, 2)

fb_only <- with_fb |> 
  filter(farm_bill == 1) 

n_fb <- length(unique(fb_only$propertyID))
fb_left <- round(n_fb / n_farm1 * 100, 2)

all_take <- dat_mis |>
    filter(start.date >= lubridate::ymd("2014-01-01")) |>
    mutate(cnty_name = if_else(grepl("ST ", cnty_name), gsub("ST ", "ST. ", cnty_name), cnty_name),
           cnty_name = if_else(grepl("KERN", cnty_name), "KERN", cnty_name))

data_mis <- all_take |>
  mutate(property_area_km2 = property.size / 247.1) |>
  filter(property_area_km2 >= 1.8,
         st_name != "HAWAII") |>
  mutate(effort = if_else(cmp_name %in% c("TRAPS, CAGE", "SNARE"), cmp.days, cmp.hours),
         effort_per = effort / cmp.qty,
         cmp_name = if_else(cmp_name == "TRAPS, CAGE", "TRAPS", cmp_name)) |>
  rename(method = cmp_name,
         trap_count = cmp.qty) |>
  select(-wt_work_date, -hours, -cmp.hours, -cmp.days) |>
  distinct() |>
  mutate(propertyID = paste0(agrp_prp_id, "-", alws_agrprop_id)) |> 
  arrange(agrp_prp_id, start.date, end.date)

# create PP of length [interval]
data_repo <- "C:/Users/John.Foster/OneDrive - USDA/Desktop/fosteR/pigs-statistical/data"
data_timestep <- create_primary_periods(data_mis, 4, data_repo) |>
  resolve_duplicate() |>         # resolve duplicate property areas
  take_filter() |>               # remove properties with zero pigs taken
  dynamic_filter() |>            # filter out bad events & properties
  condition_first_capture() |>   # condition on first positive removal event for each property
  dynamic_filter() |>            # filter out bad events & properties
  order_interval() |>            # determine midpoints from start/end dates
  order_stochastic() |>          # randomly order events
  order_of_events() |>           # assign order number, check
  county_codes()                 # county codes and renaming

# now we have two columns for time
# primary_period is how [interval] sequences are aligned across the data set
# timestep is the sequence of primary periods within a property
timestep_df <- create_timestep_df(data_timestep)

data_pp <- left_join(data_timestep, timestep_df,
                     by = join_by(propertyID, primary_period)) |>
  mutate(primary_period = primary_period - min(primary_period) + 1) |> 
  left_join(farm_bill_properties)

n_final_props <- length(unique(data_pp$propertyID))
n_final_props_fb <- data_pp |> 
  filter(farm_bill == 1) |> 
  pull(propertyID) |> 
  unique() |> 
  length()


per_left_final <- round(n_final_props / n_possible * 100, 2)
per_left_final_fb <- round(n_final_props_fb / n_farm2 * 100, 2)

```

After processing each method, there are `r n_processed` total properties, which includes `r n_fb` Farm Bill properties.

  - These properties represent `r process_left`% of all possible properties and `r fb_left`% of all Farm Bill properties. How these breakdown by state is presented in @fig-processed
  
  The final set of processing involves selecting properties that meet the minimum criteria for density estimation using a removal model.

  1. The time series is broken into 4-week chunks, or primary periods, and a primary period must have at least two removal events within it. I.e. any primary period with one event was discarded.
  
  2. We condition on first positive capture. I.e. within a property, we drop all primary periods at the beginning of a property's time series that have 0 total pigs removed. 
  
  3. With bad primary periods removed, a property must have at least two good primary periods in its time series. 
  
  4. We restricted the analysis from 2014-01-01 to present.
  
  
After this processing we are left with:

- `r n_final_props` properties (`r per_left_final`% of possible properties in MIS)
- `r n_final_props_fb` Farm Bill properties (`r per_left_final_fb`% of possible Farm Bill properties in MIS)


## MIS data overview

```{r}
#| label: fig-eventsPerProperty
#| fig-cap: "The distribution of how many pig removal events were recorded in each property."
df_events <- dat.Agr2 |>
  count(propertyID) |> 
  mutate(subset = "All properties")

df_events2 <- dat.Agr2 |>
  filter(farm_bill == 1) |> 
  count(propertyID) |> 
  mutate(subset = "Farm Bill properties")

my_theme <- function(s = 12){
  theme(
    title = element_text(size = s + 4),
    strip.text = element_text(size = s + 2),
    axis.title = element_text(size = s + 2),
    axis.text = element_text(size = s)
  )
}

rbind(df_events, df_events2) |> 
  ggplot() +
  aes(x = n) +
  geom_histogram(bins = 1500) +
  coord_cartesian(xlim = c(0, 200)) +
  labs(x = "Number of removal events",
       y = "Number of properties") +
  facet_wrap(~ subset) + 
  theme_bw() +
  my_theme()


all_q1 <- round(quantile(df_events$n, 0.25))
all_q3 <- round(quantile(df_events$n, 0.75))
fb_q1 <- round(quantile(df_events2$n, 0.25))
fb_q3 <- round(quantile(df_events2$n, 0.75))

```

We can see that the most common number of pig removal events recorded at a property is one, which is also true for the Farm Bill properties. However, 50% of all properties record between `r all_q1` and `r all_q3` events, while 50% of the Farm Bill properties record between `r fb_q1` and `r fb_q3` events.


```{r}
#| label: fig-eventsPerState
#| fig-cap: "The total number of pig removal events that were recorded in each state."

thresh <- 15
df_events <- dat.Agr2 |>
  count(ST_NAME) |> 
  filter(n > thresh) |>
  mutate(subset = "All properties") 

df_events2 <- dat.Agr2 |>
  filter(farm_bill == 1) |> 
  count(ST_NAME) |>
  filter(n > thresh) |>
  mutate(subset = "Farm Bill properties")

bind_rows(df_events, df_events2) |> 
  ggplot() +
  aes(y = reorder(ST_NAME, -n), x = n) +
  geom_col() +
  facet_wrap(~ subset) +
  labs(x = "Number of removal events",
       y = "") +
  scale_x_continuous(breaks = seq(0, 100000, length.out = 3)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  my_theme(10) +
  theme(axis.text.y = element_text(size = 10))

low_states <- dat.Agr2 |>
  count(ST_NAME) |> 
  filter(n <= thresh) |> 
  pull(ST_NAME)


```



Note that the following states had less than `r thresh` properties and were left off the graph for visualization purposes: `r low_states`

Texas has the most number of events recorded in the MIS data by far. However, when looking at the Farm Bill properties the distribution across states is more even, with Texas and Louisiana having the most removal events.


```{r}
#| label: fig-propertiesPerState
#| fig-cap: "The total number of properties with pig removal events that were recorded in each state."

thresh <- 10
df_events_p <- dat.Agr2 |>
  select(ST_NAME, propertyID) |>
  distinct() |> 
  count(ST_NAME) |> 
  filter(n > thresh) |>
  mutate(subset = "All properties") 

df_events_p2 <- dat.Agr2 |>
  filter(farm_bill == 1) |> 
  select(ST_NAME, propertyID) |>
  distinct() |> 
  count(ST_NAME) |> 
  filter(n > thresh) |>
  mutate(subset = "Farm Bill properties") 

df <- bind_rows(df_events_p, df_events_p2) 

df |> 
  ggplot() +
  aes(y = reorder(ST_NAME, -n), x = n) +
  geom_col() +
  facet_wrap(~ subset) +
  labs(x = "Number of properties",
       y = "") +
  theme_bw() +
  my_theme(10) +
  theme(axis.text.y = element_text(size = 10))

low_states <- dat.Agr2 |>
 select(ST_NAME, propertyID) |>
  distinct() |> 
  count(ST_NAME) |> 
  filter(n <= thresh) |> 
  pull(ST_NAME)

```


The following states had less than `r thresh` properties and were left off the graph for visualization purposes: `r low_states`

Like with the number of events, Texas has the most number of properties recorded in the MIS data. However, when looking at the Farm Bill properties the distribution across states is more even.


```{r}
#| label: fig-timeseries
#| fig-cap: "The total number of pig removal events that were recorded in each calendar month."

df1 <- dat.Agr2 |> 
  mutate(time = dmy(WT_WORK_DATE),
         month = month(time),
         year = year(time),
         yrm = paste0(year, "-", month)) |> 
  group_by(yrm) |>
  count() |> 
  ungroup() |> 
  mutate(yrm2 = ym(yrm),
         subset = "All properties")

df2 <- dat.Agr2 |> 
  filter(farm_bill == 1) |> 
  mutate(time = dmy(WT_WORK_DATE),
         month = month(time),
         year = year(time),
         yrm = paste0(year, "-", month)) |> 
  group_by(yrm) |>
  count() |> 
  ungroup() |> 
  mutate(yrm2 = ym(yrm),
         subset = "Farm Bill properties")

bind_rows(df1, df2) |> 
  ggplot() +
  aes(x = yrm2, y = n) +
  geom_point() +
  labs(x = "Date", 
       y = "Number of events per calender month") +
  facet_wrap(~ subset) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.6)) +
  my_theme()
```

Across the entire time series, the general trend is an increase in the number of removal events through time, and this is largely driven by the fast increase in the number of events recorded in Farm Bill properties.


### Properties in MIS that can be modeled with confidence

We conducted an extensive simulation study to determine which data qualities lead to confident density predictions. The simulation study included over 35000 properties with varying levels of take, property area, time series length, and number of observations.

We assessed how each property did at predicting known density, and then pulled out the properties that meet the criteria of making confident prediction.

```{r}
#| label: metrics

thresh_mpe <- 50
thresh_nrmse <- 1
thresh_rmse <- 0.5
thresh_bias <- 0.5

```

Those criteria are:

  1. Mean percentage error less than or equal to `r thresh_mpe`%
  
  2. Normalized root mean squared error less than or equal to `r thresh_nrmse`
  
  3. Root mean squared error less than or equal to `r thresh_rmse` pigs per square km
  
  4. Mean bias +/- `r thresh_bias` pigs per square km
  
When we subset the simulated properties to those that meet the above criteria, we see that properties must have the characteristics in @tbl-confidentProperties.

```{r}
#| label: tbl-confidentProperties
#| tbl-cap-location: bottom
#| tbl-cap: "Property characteristics that lead to confident density predictions. Property area is the area of the property in square km. Total # pigs removed is the total number of pigs harvested in a property across all removal events. Total # removal events is the total number of individual removal events recorded at a property. Total # observed PPs is the total number of primary periods with at least two removal events. Time series length is the total number of primary periods a property's data spans. Proportion of time series observed is the proportion of the total number of primary periods a property's data spans that have removal events. Total effort is the sum total of hours spent searching, regardless of method, across all removal events. Total number of units deployed is the total number of units (traps, snares, helicopters, etc) that were deployed at a property across all removal events."

acceptable_metrics <- read_csv("../pigs-simulation/analysis/betaSurvival_uniqueAreaTrapSnare/acceptable_metrics.csv")

acceptable_metrics |> 
  mutate(Metric = c("Property area (sq. km)", "Total # pigs removed", "Total # removal events", "Total # observed PPs", "Time series length", "Proportion of time series observed", "Total effort (hours)", "Total number of units deployed")) |> 
  select(Metric, min, max) |> 
  mutate(min = round(min, 3),
         max = round(max)) |> 
  kable()

```

```{r}
#| label: simulation filter

get_metric <- function(df, m, x){
  tmp <- df |> filter(metric == m) |> pull(x)
  if(x == "max") return(round(tmp))
  if(x == "min") return(round(tmp, 2))
}


area_min <- get_metric(acceptable_metrics, "property_area", "min")
area_max <- get_metric(acceptable_metrics, "property_area", "max")
take_min <- get_metric(acceptable_metrics, "take", "min")
take_max <- get_metric(acceptable_metrics, "take", "max")
n_total_events_min <- get_metric(acceptable_metrics, "n_total_events", "min")
n_total_events_max <- get_metric(acceptable_metrics, "n_total_events", "max")
n_observed_pp_min <- get_metric(acceptable_metrics, "n_observed_pp", "min")
n_observed_pp_max <- get_metric(acceptable_metrics, "n_observed_pp", "max")
ts_length_min <- get_metric(acceptable_metrics, "ts_length", "min")
ts_length_max <- get_metric(acceptable_metrics, "ts_length", "max")
prop_obs_min <- get_metric(acceptable_metrics, "proportion_observed", "min")
prop_obs_max <- get_metric(acceptable_metrics, "proportion_observed", "max")


observed_pp <- dat.Agr2 |> 
  mutate(time = dmy(WT_WORK_DATE),
         month = month(time),
         year = year(time),
         yrm = paste0(year, "-", month)) |> 
  select(propertyID, yrm) |>
  distinct() |>
  count(propertyID) |>
  rename(n_observed_pp = n)


ts_length <- dat.Agr2 |>
  mutate(time = dmy(WT_WORK_DATE)) |>
  select(propertyID, time) |>
  distinct() |>
  group_by(propertyID) |>
  filter(time == min(time) |
           time == max(time)) |>
  arrange(propertyID, time) |>
  mutate(ts_length = c(0, diff(time) + 1)) |>
  ungroup() |>
  filter(ts_length != 0) |> 
  mutate(ts_length = floor(ts_length / 28)) |> 
  select(-time)


property_char <- dat.Agr2 |>
  mutate(property_area_km2 = round(PRPS_QTY * 0.00404686, 2)) |>
  group_by(propertyID, property_area_km2) |>
  summarise(
    take = sum(WKR_QTY),
    n_total_events = n()
  ) |>
  ungroup() |>
  left_join(ts_length) |>
  left_join(observed_pp) |>
  mutate(prop_observed = n_observed_pp / ts_length) |> 
  filter(!is.na(ts_length))



n_area <- property_char |>
  filter(property_area_km2 >= area_min,
         property_area_km2 <= area_max) |> 
  nrow()
n_area_per <- round(n_area / n_possible * 100)

n_take <- property_char |>
  filter(take >= take_min,
         take <= take_max) |> 
  nrow()
n_take_per <- round(n_take / n_possible * 100)

n_total_event <- property_char |>
  filter(n_total_events >= n_total_events_min,
         n_total_events <= n_total_events_max) |> 
  nrow()
n_total_event_per <- round(n_total_event / n_possible * 100)

n_observed_pp <- property_char |>
  filter(n_observed_pp >= n_observed_pp_min,
         n_observed_pp <= n_observed_pp_max) |>
  nrow()
n_observed_pp_per <- round(n_observed_pp / n_possible * 100)

n_ts_length <- property_char |>
  filter(ts_length >= ts_length_min,
         ts_length <= ts_length_max) |>
  nrow()
n_ts_length_per <- round(n_ts_length / n_possible * 100)

n_prop_observed <- property_char |>
  filter(prop_observed >= prop_obs_min) |>
  nrow()
n_prop_observed_per <- round(n_prop_observed / n_possible * 100)


n_all <- property_char |>
  filter(property_area_km2 >= area_min,
         property_area_km2 <= area_max) |>
  filter(take >= take_min,
         take <= take_max) |>
  filter(n_total_events >= n_total_events_min,
         n_total_events <= n_total_events_max) |>
  filter(n_observed_pp >= n_observed_pp_min,
         n_observed_pp <= n_observed_pp_max) |>
  filter(ts_length >= ts_length_min,
         ts_length <= ts_length_max) |>
  filter(prop_observed >= prop_obs_min) |>
  nrow()
n_all_per <- round(n_all / n_possible * 100)
```


The following number of properties meet the min/max criteria from @tbl-confidentProperties:

  - `r n_area` (`r n_area_per`%) properties meet the area criteria
  - `r n_take` (`r n_take_per`%) properties meet the take criteria
  - `r n_total_event` (`r n_total_event_per`%) properties meet the total number of events criteria
  - `r n_observed_pp` (`r n_observed_pp_per`%) properties meet the total number of observed primary period criteria
  - `r n_ts_length` (`r n_ts_length_per`%) properties meet the time series length criteria
  - `r n_prop_observed` (`r n_prop_observed_per`%) properties meet the proportion of primary periods observed criteria
  - `r n_all` (`r n_all_per`%) properties meet all the criteria.

It is important to note that the above filtering was completed on the unprocessed MIS data, which does not have standardized effort across methods or removal events placed into 4-week primary periods. As a proxy for primary periods I have grouped removal events by year-month.

{{< pagebreak >}}

## MIS Data Processing

Now we will walk through how the MIS data is processed/cleaned into a data set that is useful for modeling. 


```{r}
#| label: method filter

trap.vec <- c("TRAPS, LIVE, FERAL HOGS","TRAPS, CAGE","TRAPS, CORRAL", "RAMIK MINI BARS :HI-9 (ANTICOAG)","TRAPS, SNAP (RAT, MOUSE, ETC.)")
firearm.vec <- c("SPOTLIGHT","CALLING DEVICE, MANUAL(HAND,BLOWN)","NIGHT VISION/INFRARED EQUIPMENT","CALLING DEVICE, ELECTRONIC","BAIT STATION","MONITORING CAMERA","CAR/TRUCK","TELEMETRY EQUIPMENT","BAIT STATION", "FIREARMS")
aerial.vec <- c("HELICOPTER","FIXED WING")
snare.vec <- c("SNARES, NECK","SNARES, FOOT/LEG","TRAPS, FOOTHOLD","TRAPS, BODY GRIP","SNARES, NECK MECHANICAL (COLLARUM)","TRAPS, FOOTHOLD (PADDED)","TRAPS, FOOTHOLD DOG PROOF")

all_methods <- c(trap.vec, firearm.vec, aerial.vec, snare.vec)

methods <- unique(dat.Agr.take$CMP_NAME)
n_methods <- length(methods)

dat.Eff2 <- dat.Agr2 |>   
  filter(CMP_NAME %in% all_methods)

n_prop_known_method <- length(unique(dat.Eff2$propertyID))
n_prop_known_method_fb <- dat.Eff2 |> 
  filter(farm_bill == 1) |> 
  pull(propertyID) |> 
  unique() |> 
  length()

method_loss <- round((n_possible - n_prop_known_method) / n_possible * 100, 2)
method_loss_fb <- round((n_farm2 - n_prop_known_method_fb) / n_farm2 * 100, 2)




```



In the MIS data, there are `r n_methods` unique methods of pig harvesting recorded. However, we are only interested in records that can be classified as on of these five: 

1. firearms (e.g. hunting)
2. fixed wing aircraft
3. helicopters
4. snares
5. traps 

These method classes were chosen because their effort (how many units deployed for how many hours) can be easily standardized, which is required for density estimation. Therefore, we only use records that have a method recorded that can be matched to one of the five listed above. This winnows that data to `r n_prop_known_method` properties, which includes `r n_prop_known_method_fb` Farm Bill properties.

- Subsetting that data to these five methods causes a loss of `r method_loss`% of total properties, and a loss of `r method_loss_fb`% of Farm Bill properties

Then, for each method, there are a series of checks to determine whether or not a pig removal event should be kept, altered, or removed:

- For traps and snares, removal events were dropped if we could not determine ff the trap/snare was checked or reset.

- For traps and snares, events lasting longer than 90 trap nights were broken in to smaller events because it is unlikely that traps and snares were not reset during this time. After breaking up these events we checked again for traps/snares being left open or not reset.

- For all records, effort data that was recorded as 0 or was missing was discarded. This includes method, number of units deployed, or search time.

  - Absolute search times that were obviously too long (i.e. weeks) were treated as a data entry error and discarded.  


  
{{< pagebreak >}}

```{r}
#| label: fig-processed
#| fig-cap: "The number properties remaining in each state after data processing"

thresh <- 10
df_events_p <- with_fb |>
  select(st_name, propertyID) |>
  distinct() |> 
  count(st_name) |> 
  filter(n > thresh) |>
  mutate(subset = "All properties") 

df_events_p2 <- with_fb |>
  filter(farm_bill == 1) |> 
  select(st_name, propertyID) |>
  distinct() |> 
  count(st_name) |> 
  filter(n > thresh) |>
  mutate(subset = "Farm Bill properties") 

df <- bind_rows(df_events_p, df_events_p2) 

df |> 
  ggplot() +
  aes(y = reorder(st_name, -n), x = n) +
  geom_col() +
  facet_wrap(~ subset) +
  labs(x = "Number of properties",
       y = "") +
  theme_bw() +
  my_theme(10) +
  scale_x_continuous(breaks = c(0, 2000, 4000)) +
  theme(axis.text.y = element_text(size = 10))

low_states <- with_fb |>
 select(st_name, propertyID) |>
  distinct() |> 
  count(st_name) |>  
  filter(n <= thresh) |> 
  pull(st_name)


```

The following states had less than `r thresh` properties and were left off the graph for visualization purposes: `r low_states`

### Processed data to model data


```{r}
#| label: process model data
#| results: hide




observed_pp <- data_pp |>
    select(propertyID, primary_period) |>
    distinct() |>
    count(propertyID) |>
    rename(n_observed_pp = n)

ts_length <- get_ts_length(data_pp) |>
  rename(ts_length = delta) |>
  select(-primary_period)

best_properties <- data_pp |>
  group_by(propertyID, property_area_km2) |>
  summarise(take = sum(take),
            effort = sum(effort_per),
            unit_count = sum(trap_count),
            n_total_events = n()) |>
  ungroup() |>
  left_join(ts_length) |>
  left_join(observed_pp) |>
  mutate(prop_observed = n_observed_pp / ts_length) |>
  filter(property_area_km2 >= area_min, property_area_km2 <= area_max,
         take >= take_min, take <= take_max,
         n_total_events >= n_total_events_min, n_total_events <= n_total_events_max,
         n_observed_pp >= n_observed_pp_min, n_observed_pp <= n_observed_pp_max,
         ts_length >= ts_length_min, ts_length <= ts_length_max,
         prop_observed >= prop_obs_min)

n_best <- nrow(best_properties)

# farm_bill_properties <- farm_bill_properties |> 
#   rename(alws_agrprop_id = ALWS_AGRPROP_ID)

data_for_join <- data_pp |> 
  select(propertyID, agrp_prp_id, alws_agrprop_id) |> 
  unique() |> 
  left_join(farm_bill_properties)

n_best_fb <- left_join(best_properties, data_for_join) |> 
  filter(farm_bill == 1) |> 
  nrow()

```



If we apply the property characteristics from @tbl-confidentProperties to the final processed data, we are left with `r n_best` properties that we are confident in estimating their density, of which `r n_best_fb` are Farm Bill properties.

Below are the same figures from above (the raw MIS data), but now using the data suitable for modeling (the `r n_final_props` data set). 

```{r}
#| label: fig-eventsPerPropertyFinal
#| fig-cap: "The distribution of removal events in each property in the final data"
df_events <- data_pp |>
  count(agrp_prp_id) |> 
  mutate(subset = "All properties")

df_events2 <- data_pp |>
  filter(farm_bill == 1) |> 
  count(agrp_prp_id) |> 
  mutate(subset = "Farm Bill properties")

rbind(df_events, df_events2) |> 
  ggplot() +
  aes(x = n) +
  geom_histogram(bins = 1500) +
  coord_cartesian(xlim = c(0, 200)) +
  labs(x = "Number of removal events",
       y = "Number of properties") +
  facet_wrap(~ subset) + 
  theme_bw() +
  my_theme()
```


{{< pagebreak >}}


```{r}
#| label: fig-eventsPerStateFinal
#| fig-cap: "The total number of pig removal events that were recorded in each state in the final data set. All states shown."

thresh <- 0
df_events <- data_pp |>
  count(st_name) |> 
  filter(n > thresh) |>
  mutate(subset = "All properties") 

df_events2 <- data_pp |>
  filter(farm_bill == 1) |> 
  count(st_name) |>
  filter(n > thresh) |>
  mutate(subset = "Farm Bill properties")

bind_rows(df_events, df_events2) |> 
  ggplot() +
  aes(y = reorder(st_name, -n), x = n) +
  geom_col() +
  facet_wrap(~ subset) +
  labs(x = "Number of removal events",
       y = "") +
  scale_x_continuous(breaks = seq(0, 50000, length.out = 3)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
  my_theme(10) +
  theme(axis.text.y = element_text(size = 10))
```

{{< pagebreak >}}


```{r}
#| label: fig-propertiesPerStateFinal
#| fig-cap: "The total number of properties with pig removal events that were recorded in each state in the final data set. All state shown."

df_events_p <- data_pp |>
  select(st_name, agrp_prp_id) |>
  distinct() |> 
  count(st_name) |> 
  filter(n > thresh) |>
  mutate(subset = "All properties") 

df_events_p2 <- data_pp |>
  filter(farm_bill == 1) |> 
  select(st_name, agrp_prp_id) |>
  distinct() |> 
  count(st_name) |> 
  filter(n > thresh) |>
  mutate(subset = "Farm Bill properties") 

df <- bind_rows(df_events_p, df_events_p2) 

df |> 
  ggplot() +
  aes(y = reorder(st_name, -n), x = n) +
  geom_col() +
  facet_wrap(~ subset) +
  labs(x = "Number of properties",
       y = "") +
  theme_bw() +
  my_theme(10) +
  theme(axis.text.y = element_text(size = 10))

```


{{< pagebreak >}}

```{r}
#| label: fig-timeseriesFinal
#| fig-cap: "The total number of pig removal events that were recorded in each calendar month in the final data set."


end_dates <- unique(sort(data_mis$end.date))
min_date <- min(end_dates)
max_date <- max(end_dates)

start_dates <- seq(min_date, max_date, by = paste(4, "week"))
end_dates <- c(start_dates[-1] - 1, max_date)

targets::tar_assert_identical(length(start_dates), length(end_dates))
targets::tar_assert_true(min(df$start.date) >= min_date)
targets::tar_assert_true(max(df$start.date) <= max_date)

timestep_df <- tibble(start_dates, end_dates) |>
  mutate(primary_period = 1:n())
timestep_df$month <- month(timestep_df$end_dates)
timestep_df$year <- year(timestep_df$end_dates)

df_with_dates <- left_join(data_pp, timestep_df)

df1 <- df_with_dates |> 
  mutate(yrm = paste0(year, "-", month)) |> 
  group_by(yrm) |>
  count() |> 
  ungroup() |> 
  mutate(yrm2 = ym(yrm),
         subset = "All properties")

df2 <- df_with_dates |> 
  filter(farm_bill == 1) |> 
  mutate(yrm = paste0(year, "-", month)) |> 
  group_by(yrm) |>
  count() |> 
  ungroup() |> 
  mutate(yrm2 = ym(yrm),
         subset = "Farm Bill properties")

bind_rows(df1, df2) |> 
  ggplot() +
  aes(x = yrm2, y = n) +
  geom_point() +
  labs(x = "Date", 
       y = "Number of events per calender month") +
  facet_wrap(~ subset) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.6)) +
  my_theme()
```


